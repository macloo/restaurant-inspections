{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installed with pip\n",
    "from pandas import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from collections import namedtuple\n",
    "\n",
    "# built-in libraries\n",
    "import datetime\n",
    "import re\n",
    "import sqlite3\n",
    "import urllib\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read state summary report for District into Pandas df; filter for needed fields and assign headers\n",
    "insp = pd.read_csv(\"ftp://dbprftp.state.fl.us/pub/llweb/5fdinspi.csv\", \n",
    "                   usecols=[2, 4, 5, 6, 7, 8, 9, 12, 13, 14, 17, 18, 80, 81], encoding=\"ISO-8859-1\")\n",
    "insp.columns = [\"county\", \"licnum\", \"sitename\", \"streetaddy\", \"cityaddy\", \"zip\",\n",
    "                \"inspnum\", \"insptype\", \"inspdispos\", \"inspdate\", \"totalvio\", \"highvio\", \"licid\", \"visitid\"]\n",
    "county_sought = 'Marion' # uncomment if particular county sought\n",
    "insp = insp[(insp.county == county_sought)] # uncomment if particular county sought\n",
    "insp.sitename = insp.sitename.str.title() #titlecase sitename\n",
    "insp.streetaddy = insp.streetaddy.str.title() #titlecase street adress\n",
    "insp.cityaddy = insp.cityaddy.str.title() #titlecase city\n",
    "insp = insp.applymap(lambda x: str(x).strip() if len(str(x).strip()) else None) #strip whitespace, replace empty vals\n",
    "insp['visitid'] = insp['visitid'].apply(int) # so it can be filtered against df below\n",
    "\n",
    "# Read in records from database of earlier reports, create df to filter against new reports in df above.\n",
    "conn = sqlite3.connect(\"rinspect.sqlite\")\n",
    "df = pd.read_sql_query(\"select * from fdinsp;\", conn)\n",
    "df\n",
    "conn.close()\n",
    "\n",
    "unique_vals = insp[~insp.visitid.isin(df.visitid)] #filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a list of URLs to the inspectors detailed reports that will get scraped\n",
    "result = []\n",
    "result_for_urls = result # for url list\n",
    "result_for_list = result.append(\"NULL\") # later into db, space for user-input datetime\n",
    "\n",
    "# takes LicenseID and VisitID, passes it into the urls for detailed reports later\n",
    "for index, rows in unique_vals.iterrows():\n",
    "    visitid = rows['visitid']\n",
    "    licid = rows['licid']\n",
    "    urls = \"https://www.myfloridalicense.com/inspectionDetail.asp? \\\n",
    "        InspVisitID= %s &id= %s\" % (visitid, licid)\n",
    "    urls = urls.replace(' ', '')\n",
    "    result.append(urls)\n",
    "urlList = result\n",
    "urlList.pop(0) # get rid of first \"Null\" from append above\n",
    "len(urlList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place data from state summary report df into database\n",
    "# first, interate through the df to return tuples\n",
    "var = list(unique_vals.itertuples(index='visitid', name=None))\n",
    "# populate database table for inspection summmary reports\n",
    "sqlite_file = 'rinspect.sqlite'\n",
    "fdinsp_table = 'fdinsp' # table for summary data\n",
    "id_field = 'visitid' # ID column\n",
    "time_now = 'now' # column for user-input timestamp; until then = NULL\n",
    "\n",
    "# connect to database and insert new summary report data\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "c.executemany('''INSERT OR IGNORE INTO fdinsp (librow, county, licnum, sitename,\n",
    "              streetaddy, cityaddy, zip, inspnum, insptype, inspdispos,\n",
    "              inspdate, totalvio, highvio, licid, visitid)\n",
    "              VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', var)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlList = ['https://www.myfloridalicense.com/inspectionDetail.asp?InspVisitID=6632858&id=6605529', 'https://www.myfloridalicense.com/inspectionDetail.asp?InspVisitID=6653464&id=4070634', 'https://www.myfloridalicense.com/inspectionDetail.asp?InspVisitID=6602877&id=4070634', 'https://www.myfloridalicense.com/inspectionDetail.asp?InspVisitID=6577708&id=2129302',]# a shorter try for a list\n",
    "url = 'https://www.myfloridalicense.com/inspectionDetail.asp?InspVisitID=6632858&id=6605529'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visitid': '6632858',\n",
       " 'observations': '[\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tBasic - Carbon dioxide/helium tanks not adequately secured.\\n Observed 2 compressed gas tanks on the wait line, not secured. The chain was added. **Corrected On-Site**\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tBasic - Case/container/bag of food stored on floor in dry storage area.\\n Observed bulk oil stored on the floor in dry storage.\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tBasic - In-use utensil stored in standing water less than 135 degrees Fahrenheit.\\n Observed rice spoon in 84° water on the cook line. The water was discarded. **Corrected On-Site*\\n ALSO, tongs were on equipment handles, protruding into the cook line walkway. The tongs were moved. **Corrected On-Site**\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tBasic - In-use wiping cloth/towel used under cutting board.\\n Observed cloth under cutting board. The towel was removed. **Corrected On-Site**\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tBasic - Soiled cloths, linen, aprons, coats, or other uniform apparel not kept in a suitable container prior to laundering.\\n Observed soiled apron stored on dry good shelves, over prep area. The apron was placed in a soiled linen bag.\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tBasic - Walk-in cooler gasket torn/in disrepair.\\n Observed the walk-in cooler door gasket is compromised, and repaired with duct tape.\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tHigh Priority - Dishmachine chlorine sanitizer not at proper minimum strength. Discontinue use of dishmachine for sanitizing and set up manual sanitization until dishmachine is repaired and sanitizing properly.\\n Observed the dish wash machine at 0 ppm for chlorine. The new bucket was primed and then the dish wash machine achieved 50 ppm at 140°. **Corrected On-Site**\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tIntermediate - Handwash sink not accessible for employee use due to being blocked by garbage can.\\n Observed  on the cook line, the hand wash sink was blocked by stock pots. The pots were moved.\\n ALSO, there was a fry pan in the sink. It was removed. **Corrected On-Site**\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tIntermediate - Ready-to-eat, potentially hazardous (time/temperature control for safety) food prepared onsite and held more than 24 hours not properly date marked.\\n Observed cut portion of sushi fish in the reach-in cooler on the sushi line. No date. A date was added to the wrapper. **Corrected On-Site**\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t\\t\\tIntermediate - Required employee training expired for some employees. To order approved program food safety material, call DBPR contracted provider:  Florida Restaurant and Lodging Association (SafeStaff) 866-372-7233.\\n Observed 1 expired certificate fir employees.]'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gets observations details from a single url\n",
    "obs = {}\n",
    "file_path = 'observations.txt'\n",
    "visitid = url.split(\"VisitID=\")[1].split(\"&\")[0]\n",
    "html = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "details = soup.findAll('font', {'face': 'verdana'})[42:]\n",
    "details = str(details)\n",
    "# regex string matches for tags in the soup; not sure if to replace here or later\n",
    "new_details = re.sub(\"\\<.*\\>\", \"\", details)\n",
    "\n",
    "obs['visitid'] = visitid\n",
    "obs['observations'] = new_details\n",
    "# writes out file\n",
    "# need to append with visitid, and other details from database, into narrative;\n",
    "# put observations into the database? If so, need to separate them into different fields\n",
    "\n",
    "#with open('observations.txt'.format(file_path), mode='wt', encoding='utf-8') as file:\n",
    "#    file.write(str(new_details))\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or take a different approach by building a dictionary of observations;\n",
    "# but this isn't close to working\n",
    "# function to scrape inspections from list of urls above, place in data structure\n",
    "\n",
    "\n",
    "def make_details_dict(url):\n",
    "    global detailsdict\n",
    "    inspurl = urlList\n",
    "    # extract VisitID number from url; will be pk in database\n",
    "    visitid.append(inspurl.split(\"VisitID=\")[1].split(\"&\")[0]) \n",
    "    # or ...\n",
    "    # visitid = inspurl[inspurl.rfind(\"VisitID=\") + 1:8]\n",
    "    html = urlopen(url)\n",
    "    soup = bs(html.read(), 'lxml')\n",
    "    details = soup.find_all('font', {'face': 'verdana'})[10:] # relevant text on pages in verdana\n",
    "    detailsdict = {\n",
    "        'visitid' : visitid,\n",
    "        'observed': details,\n",
    "    }\n",
    "    result.append(detailsdict)    \n",
    "    return details_dict\n",
    "\n",
    "# iterate through urls, calling function above to make dictionary\n",
    "for url in urlList:\n",
    "    make_details_dict(url)\n",
    "                          \n",
    "# need to send this stuff to dict once it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'rfind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d6e95df57239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murlList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmake_details_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d6e95df57239>\u001b[0m in \u001b[0;36mmake_details_list\u001b[0;34m(urlList)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_details_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minspurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mvisitid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minspurl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VisitID=\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'rfind'"
     ]
    }
   ],
   "source": [
    "# a different try for a dictionary; do I need to work with classes to build this?\n",
    "# the exceptions seem to work ok, but will need \"Nulls\" not None into database if we do that.\n",
    "details_dict = {}\n",
    "def make_details_list(urlList):\n",
    "    inspurl = urlList\n",
    "    visitid = visitid.append(inspurl.split(\"VisitID=\")[1].split(\"&\")[0])\n",
    "    html = urlopen(url)\n",
    "    soup = bs(html.read(), 'lxml')\n",
    "    details = soup.find_all('font', {'face': 'verdana'})[10:] # relevant text on pages in verdana\n",
    "    result = []\n",
    "        \n",
    "    try:\n",
    "        observed1 = details[32].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed1\n",
    "    try:\n",
    "        observed2 = details[34].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed2\n",
    "    try:\n",
    "        observed3 = details[36].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed3\n",
    "    try:\n",
    "        observed4 = details[38].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed4\n",
    "    try:\n",
    "        observed5 = details[40].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed5\n",
    "    try:\n",
    "        observed6 = details[42].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed6\n",
    "    try:\n",
    "        observed7 = details[44].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed7\n",
    "    try:\n",
    "        observed8 = details[46].text\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    return observed8\n",
    "        \n",
    "    result.append(details_list)\n",
    "    return details_list    \n",
    "\n",
    "for url in urlList:\n",
    "    make_details_list(urlList)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
